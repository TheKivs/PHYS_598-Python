\documentclass[aps,prl,groupedaddress,amsmath,amssymb,nofootinbib,12pt]{revtex4-1}
\pdfoutput=1
\oddsidemargin -40pt
 \evensidemargin -40pt
\hyphenpenalty=10000
\usepackage[
top    = 2.1cm,
bottom = 2 cm,
left   = 1.9cm,
right  = 1.9cm]{geometry}
\usepackage[utf8]{inputenc}

\def\thesection{\Roman{section}}

%\usepackage{mathrsfs}
\usepackage[usenames, dvipsnames]{color}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{slashed}
\usepackage[colorlinks=true,linkcolor=ForestGreen, citecolor=ForestGreen]{hyperref}

\usepackage{etoolbox}

%\usepackage[notref,notcite]{showkeys}




%\usepackage[notref,notcite]{showkeys}
%\usepackage{showkeys}





\usepackage{graphicx}
\usepackage{bm}


\linespread{1.196}


\definecolor{davecolor}{rgb}{0.95,  0.5,  0.2}
\def\DV#1{{\color{davecolor}{\bf #1}}}

\def\del{\partial}

\def\eg{{\it e.g.}}
\def\ie{{\it i.e.}}
\def\cf{{\it c.f. }}
\def\etal{{\it et. al.}}



\def\({\left(}
\def\){\right)}

\def\<{\langle}
\def\>{\rangle}
%\def\half{{1\over 2}}

\def\CA{{\cal A}}
\def\CC{{\cal C}}
\def\CD{{\cal D}}
\def\CE{{\cal E}}
\def\CF{{\cal F}}
\def\CG{{\cal G}}
\def\CT{{\cal T}}
\def\CM{{\cal M}}
\def\CN{{\cal N}}
\def\CO{{\cal O}}%AEL
\def\CP{{\cal P}}
\def\CL{{\cal L}}
\def\CV{{\cal V}}
\def\CS{{\cal S}}
\def\CW{{\cal W}}
\def\CX{{\cal X}}%AEL
%\def\bra#1{{\langle}#1|}
%\def\ket#1{|#1\rangle}
\def\bbra#1{{\langle\langle}#1|}
\def\kket#1{|#1\rangle\rangle}
%\def\vev#1{\langle{#1}\rangle}
\def\Dslash{\rlap{\hskip0.2em/}D}
\def\CDslash{\rlap{\hskip0.2em/}{\CD}}

\def\DDslash{{\cal L}}
%\def\Dslash{\rlap{\hskip0.2em/}D}
%\def\vev#1{\langle#1 \rangle}
%\def\CO{{\cal O}}
%\def\half{{1\over 2}}


\newcommand{\cit}[1]{  [{\bf\texttt{#1}}]}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother



\def\Tr{\mathop{\rm Tr}}
\def\tr{\mathop{\rm tr}}

\newcommand\half{{\ensuremath{\frac{1}{2}}}}
\newcommand\p{\ensuremath{\partial}}
\newcommand\evalat[2]{\ensuremath{\left.{#1}\right|_{#2}}}
\newcommand\abs[1]{\ensuremath{\left\lvert{#1}\right\rvert}}
\newcommand\no[1]{{{:}{#1}{:}}}
\newcommand\transpose{{\ensuremath{\text{\sf T}}}}
\newcommand\field[1]{{\ensuremath{\mathbb{{#1}}}}}
\newcommand\order[1]{{\ensuremath{{\mathcal O}({#1})}}}
\newcommand\vev[1]{{\ensuremath{\left\langle{#1}\right\rangle}}}
\newcommand\anti[2]{\ensuremath{\bigl\{{#1},{#2}\bigr\}}}
\newcommand\com[2]{\ensuremath{\bigl[{#1},{#2}\bigr]}}
\newcommand\ket[1]{\ensuremath{\lvert{#1}\rangle}}
\newcommand\bra[1]{\ensuremath{\langle{#1}\rvert}}
\newcommand\lie[2]{\ensuremath{\pounds_{{#1}} {#2}}}
%\newcommand\sfrac[2]{\ensuremath{{({#1})}/{({#2})}}} % or
\newcommand\sfrac[2]{\ensuremath{\frac{#1}{#2}}}
\newcommand\lvec[2][]{\ensuremath{\overleftarrow{{#2}_{#1}}}}
\newcommand\rvec[2][]{\ensuremath{\overrightarrow{{#2}_{#1}}}}
%\newcommand\lvec[2][]{\ensuremath{\vec{{#2}}_{#1}}}
%\newcommand\rvec[2][]{\ensuremath{\stackrel{\rightarrow}{{#2}}_{#1}}}


\newcommand{\myfig}[3]{
	\begin{figure}[ht]
	\centering
	\includegraphics[width=#2cm]{figs/#1}\caption{#3}\label{fig:#1}
	\end{figure}
	}
\newcommand{\littlefig}[2]{
	\includegraphics[width=#2cm]{figs/#1}
	}


\newcommand\speceq{\ensuremath{\stackrel{\star}{=}}}
\newcommand\conj[1]{{\ensuremath{\left({#1}\right)^*}}}



%\newcommand{\field}[1]{\ensuremath{\mathbb{#1}}}
\newcommand{\A}{\field{A}}
%\newcommand{\CC}{\field{C}}
\newcommand{\DD}{\field{D}}
\newcommand{\J}{\field{J}}
\newcommand{\GG}{\field{G}}
\newcommand{\FF}{\field{F}}
\newcommand{\QQ}{\field{Q}}
\newcommand{\HH}{\field{H}}
\newcommand{\LL}{\field{L}}
\newcommand{\KK}{\field{K}}
\newcommand{\NN}{\field{N}}
\newcommand{\PP}{\field{P}}
\newcommand{\RR}{\field{R}}
\newcommand{\TT}{\field{T}}
\newcommand{\ZZ}{\field{Z}}





\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\bwt}{\begin{widetext}}
\newcommand{\ewt}{\end{widetext}}
\newcommand{\nn}{\nonumber\\}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\bca}{\begin{cases}}
\newcommand{\eca}{\end{cases}}
\newcommand{\bln}{\begin{align}}
\newcommand{\eln}{\end{align}}
\newcommand{\bst}{\begin{split}}
\newcommand{\est}{\end{split}}

\newcommand\al{{\alpha}}
\newcommand\ep{\epsilon}
\newcommand\sig{\sigma}
\newcommand\Sig{\Sigma}
\newcommand\lam{\lambda}
\newcommand\Lam{\Lambda}
\newcommand\om{\omega}
\newcommand\Om{\Omega}
\newcommand\vt{\vartheta}
\newcommand\ga{{\ensuremath{{\gamma}}}}
\newcommand\Ga{{\ensuremath{{\Gamma}}}}
\newcommand\de{{\ensuremath{{\delta}}}}
\newcommand\De{{\ensuremath{{\Delta}}}}
\newcommand\vp{\varphi}
\newcommand\ze{{\zeta}}

\newcommand\da{{\dagger}}
\newcommand\nab{{\nabla}}
\newcommand\Th{{\Theta}}
\def\th{{\theta}}

\newcommand\ra{{\rightarrow}}
\newcommand\Lra{{\Longrightarrow}}
\newcommand\ov{\over}
\newcommand\ha{{\half}}
\newcommand\papr{{2 \pi \apr}}
\newcommand\apr{{\ensuremath{{\alpha'}}}}
\def\le{\left}
\def\ri{\right}

\newcommand\sA{{\ensuremath{{\mathcal A}}}}
\newcommand\sB{{\ensuremath{{\mathcal B}}}}
\newcommand\sC{{\ensuremath{{\mathcal C}}}}
\newcommand\sD{{\ensuremath{{\mathcal D}}}}
\newcommand\sF{{\ensuremath{{\mathcal F}}}}
\newcommand\sI{{\ensuremath{{\mathcal I}}}}
\newcommand\sG{{\ensuremath{{\mathcal G}}}}
\newcommand\sK{{\ensuremath{{\mathcal K}}}}
\newcommand\sH{{\ensuremath{{\mathcal H}}}}
\newcommand\sL{{\ensuremath{{\mathcal L}}}}
\newcommand\sM{{\ensuremath{{\mathcal M}}}}
\newcommand\sN{{\ensuremath{{\mathcal N}}}}
\newcommand\sO{{\ensuremath{{\mathcal O}}}}
\newcommand\sR{{\ensuremath{{\mathcal R}}}}
\newcommand\sZ{{\ensuremath{{\mathcal Z}}}}
\newcommand\sn{{\ensuremath{{\mathfrak n}}}}


\newcommand\sg{g}

\newcommand\sV{{\mathcal V}}
\newcommand\sJ{{\mathcal J}}
\newcommand\sS{{\mathcal S}}

\newcommand\bQ{{\bf Q}}
\newcommand\bT{{\bf T}}
\newcommand\bA{{\bf A}}
\newcommand\bB{{\bf B}}
\newcommand\bC{{\bf C}}
\newcommand\bR{{\bf R}}
\newcommand\bX{{\bf X}}
\newcommand\bY{{\bf Y}}
\newcommand\bI{{\bf I}}
\newcommand\bv{{\bf v}}
\newcommand\bw{{\bf w}}
\newcommand\bII{{\bf II}}
\newcommand\bth{{\boldsymbol \theta}}
\newcommand\bom{{\boldsymbol \omega}}
\newcommand\bq{{\bf Q}_B}
\newcommand\bfb{{\bf b}_{-1}}

\newcommand\bc{{\bar c}}
\newcommand\bb{{\bar b}}

\newcommand\bpsi{{\bar \psi}}


\renewcommand{\Im}{\textrm{Im}\,}
\renewcommand{\Re}{\textrm{Re}\,}


\newcommand{\rd}{\ell_2}
\newcommand{\hmq}{\widehat \mu_q}
\newcommand{\bone}{{\bf 1}}
\newcommand{\vk}{{\vec k}}

\newcommand\tpi{{\tilde \pi}}

\def\tildem{\tilde m}
%\def\psis{\textswab{Y}}
\def\yandz{\Phi}
\def\psis{\yandz}

\newcommand\uz{{\underline{z}}}
\newcommand\utau{{\underline{\tau}}}
\newcommand\ut{{\underline{t}}}
\newcommand\ur{{\underline{r}}}
\newcommand\ui{{\underline{i}}}
\newcommand\uj{{\underline{j}}}
\newcommand\umu{{\underline{\mu}}}
\newcommand\uy{{\underline{y}}}

\newcommand{\dbyd}[1]{\ensuremath{\left(\frac{\partial}{\partial #1}\right)}}
\newcommand{\itoj}{\ensuremath{i\leftrightarrow j}}
\newcommand{\ux}{\underline x}


\newcommand{\uM}{\underline M}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}

%\def\Dslash{\rlap{\hskip0.2em/}D}
\def\Aslash{\rlap{\hskip0.2em/}A}
\def\pslash{\rlap{\hskip0.1em/}p}
\def\kslash{\rlap{\hskip0.1em/}k}

\newcommand\psinorm{\boldsymbol{\psi}}
\newcommand\Psinorm{\boldsymbol{\Psi}}
\newcommand\Phinorm{\boldsymbol{\Phi}}
\def\vertexZ{\Lambda}
%\def\dk{\(d\vec k\)}
\def\Q{\mathcal{Q}}

\newcommand\Psinon{\mathfrak{Y}}



\begin{document}


\section{PHYS 598 SDA RECITATION 3 - PROBLEM SET}

**(Starred problems to be done only after you're done with the unstarred problems.)**\\

\textbf{1.} Let $\mu \in \mathbb{R}$. Let $X_1$ and $X_2$ be independent random variables with distributions $N(\mu,1)$ and $N(\mu,4)$ respectively. [The notation $N(\mu,\sigma)$ is standard, where $\mu$ is the mean and $\sigma$ is the variance.] Let the variables $T_1$, $T_2$ and $T_3$ be defined by
\begin{equation}
T_1 = \frac{X_1 +X_2}{2}, ~~~~~~ T_2 = 2X_1 - X_2, ~~~~~~ T_3 = \frac{4X_1 + X_2}{5}.
\end{equation}
Find the mean and variance of $T_1$, $T_2$ and $T_3$. Which of $\mathbb{E}[T_1]$, $\mathbb{E}[T_2]$ and $\mathbb{E}[T_3]$ would you prefer to have an estimator for $\mu$? [$\mathbb{E}[Y]$ is standard notation for expectation/mean of the distribution $Y$.] 
\vspace{1 mm}


\textbf{P.S. } If you're feeling especially math-inclined : For what $(\alpha,\beta)$, with $\alpha+\beta = 1$ would $T = \alpha X_1+\beta X_2$ be the best estimator for $\mu$? \\

\textbf{2.*} Let $X$ be a $N(0, 1)$ random variable. Use integration by parts to show that 
\begin{equation}
\mathbb{E}[X^{n+2}] = (n+1)~\mathbb{E}[X^{n}] ~~ \forall n \in \mathbb{Z}^+.
\end{equation}
Reason why $\mathbb{E}[X^{n}] = 0$ when $n$ is odd; what is the general answer if $n$ is even? (Hint : probably a rare occasion to use ``!!")\\

\textbf{3.} Let $(X, Y)$ be a random vector with joint probability density function
\begin{equation}
f_{X,Y}(x,y) = ke^{-(x+y)}~~~ \text{if }0<y<x , ~~~~~~~ 0 \text{ otherwise.}
\end{equation}
\begin{itemize}
\item
Using that $\mathbb{P}[(X, Y ) \in \mathbb{R}^2]$ = 1, find the value of $k$.
\vspace{-2 mm}

\item
For the regions $S$ and $T$ in $\mathbb{R}^2$:
\[ S = \{(x,y)~ :~ x \in [0,1], ~y \in [0,1]\}, ~~~~T = \{(x,y)~ :~ 0<x<y\},\]
calculate the probability that $(X, Y)$ is inside the given region
\vspace{-2 mm}

\item
Find the marginal \emph{probability distribution function} (PDF) of $Y$, and hence identify the distribution of $Y$. (Google might be your best friend here!)
\end{itemize}

\textbf{4.} Let $(X, Y)$ be a random vector with joint probability density function.
\begin{equation}
f_{X,Y}(x,y) = \begin{cases} \frac{y-x}{2}, & x\in [-1,0], y\in [0,1];\\ \frac{x+y}{2}, & x\in [0,1], y\in [0,1];\\ 0, & \text{otherwise}; \end{cases}
\end{equation}
Find the marginal PDF of $X$. Show that the correlation coefficient $X$ and $Y$ is zero.
Show/infer that $X$ and $Y$ are not independent.\\

\textbf{5.*} Let $X$ be a random variable. Also, let $Z$ be a random variable, independent of $X$, such that $\mathbb{P}[Z =1]$ = $\mathbb{P}[Z =-1]$ = 1/2. Let $Y = XZ$. Show that $X$ and $Y$ are uncorrelated.
\pagebreak

\textbf{Solutions\\}

\textbf{1.} We have 
\[\mathbb{E}[T_1) = \frac{1}{2} (\mathbb{E}[X_1] + \mathbb{E}[X_2]) = \mathbb{E}[T_2] = \mu.\]
Similarly, $\mathbb{E}[T_2] = \mathbb{E}[T_3] = \mu$, so all are unbiased when used as estimators of $\mu$. We have 
\[\text{Var}(T_1) = \left(\frac{1}{2}\right)^2\left(\text{Var}(X_1) + 2 \text{Cov}(X_1, X_2) + \text{Var}(X_2)\right) = \frac{1}{4}(1 + 0 + 4) = 5,
\]
and similarly Var$(T_2)$ = 8, Var$(T_3)$ = 4/5. That is, we prefer $\mathbb{E}[T_3]$ as an estimator of $\mu$, because $T_3$ has the smallest variance and so is likely to be closest to its mean.\\

\textbf{2.} Since $\mathbb{E}[X]$ = 0, induction gives that $\mathbb{E}[X^n
]$ = 0 for all odd $n$. Similarly, $\mathbb{E}[X^n] = 1\times 3 \times 5 \times \ldots \times (n-1)$ when $n$ is even.\\

\textbf{3.} We just need to compute a bunch of integrals here...
\begin{itemize}
\item
Equate
\[\mathbb{P}[(X, Y ) \in \mathbb{R}^2] \equiv \int_{-\infty}^\infty\int_{-\infty}^\infty f_{X,Y} (x,y)~dxdy = 1.\]
The value of the integral is $k/2$, that is $k=2$.
\item We have
\[\mathbb{P}[(X, Y ) \in S] = \int_0^1\int_0^x 2e^{-(x+y)} = \frac{1}{e^2} -\frac{2}{e}+1,\]
Since $f_{X,Y} (x, y)$ = 0 $\forall (x, y) \in T$, $\mathbb{P}[(X, Y ) \in T]$ = 0.
\item
We integrate $x$ out, giving for $y > 0$,
\[f_Y(y) = \int_{-\infty}^\infty f_{X,Y} (x, y)dx = 2e^{-2y}.\]
So $Y$ has an \emph{exponential distribution} with parameter $\lambda$ = 2 (or, equivalently, a \emph{Gamma
distribution} with parameters $\alpha = 1$ and $\beta$ = 2).
\end{itemize}

\textbf{4.} As above, the marginal PDF is
\[f_X(x) = \int_0^1 f_{X,Y} (x, y) dy\]
which can be computed in the three cases as
\[f_{X}(x) = \begin{cases} \int_0^1\frac{y-x}{2}dx=\frac{1-2x}{4};\\ \int_0^1\frac{x+y}{2} = \frac{1+2x}{4};\\ 0, & \text{otherwise}; \end{cases}\]
This gives $\mathbb{E}[X]$ = 0, $\because$ $f_X(x) = f_X(-x)$. To find $\mathbb{E}[XY]$, we calculate
\[\mathbb{E}[XY] = \int_0^1 \int_{-1}^1 xy f_{X,Y} (x, y) dxdy = 0.\]
Hence, the covariance is $\mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y ] = 0$ (note that we do not need to know the value of $\mathbb{E}[Y]$). Hence the correlation coefficient is also zero.\\

To show dependence, we will show that the joint PDF of $X$ and $Y$ does not factorize into the marginal densities. For example, dividing $f_{X,Y} (x, y)$ by $f_X(x)$ gives $2(y+x)/(1+2x)$ if $x \in [0, 1]$ and $2(y-x)/
(1-2x)$ if $x \in [-1, 0]$, which does not depend only on $y$, so cannot be equal to $f_Y (y)$. Hence,
$X$ and $Y$ are not independent.\\

\textbf{5.} All we need to show is that Cov($X,Y$) = 0, using the fact that $\mathbb{E}(XZ) = \mathbb{E}(X)\mathbb{E}(Z)$.
\end{document}


